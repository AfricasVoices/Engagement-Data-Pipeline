from os import path
import json
from dateutil.parser import isoparse


from core_data_modules.logging import Logger
from core_data_modules.util import TimeUtils

log = Logger(__name__)


def get_project_messages_from_engagement_db(pipeline_config, engagement_db, engagement_db_datasets_export_dir):
    """

    Downloads project messages from engagement database. It performs a full download if there is no previous export and
    incrementally otherwise.

    :param pipeline_config: Pipeline configuration module.
    :type pipeline_config: PIPELINE_CONFIGURATION
    :param engagement_db: Engagement database to update the message in.
    :type engagement_db: engagement_database.EngagementDatabase
    :param engagement_db_datasets_export_dir: Directory containing engagement_db dataset files generated by a previous
                                            run of this pipeline new or changed messages will be updated to these files.
    :type engagement_db_datasets_export_dir: str
    :return: engagement_db_dataset_messages_map of engagement_db_dataset to list of messages.
    :rtype: dict
    """

    engagement_db_dataset_messages_map = {} # of engagement_db_dataset to list of messages

    for coda_dataset_config in pipeline_config.coda_sync.sync_config.dataset_configurations:

        engagement_db_dataset = coda_dataset_config.engagement_db_dataset
        previous_export_file_path = path.join(engagement_db_datasets_export_dir, f"{engagement_db_dataset}.json")

        last_updated = None
        messages = []
        new_messages = []
        if path.exists(previous_export_file_path):
            with open(previous_export_file_path) as f:
                previous_export = json.load(f)

            for msg in previous_export:
                if last_updated is None or isoparse(msg["last_updated"]) > last_updated:
                    last_updated = isoparse(msg["last_updated"])

            incremental_get_filter = lambda q: q \
                .where("dataset", "==", engagement_db_dataset) \
                .where("last_updated", ">", last_updated)

            log.info(f"Performing incremental get for {engagement_db_dataset} dataset messages")
            new_messages.extend(engagement_db.get_messages(filter=incremental_get_filter))

            # Add messages that have not been updated in engagement db
            new_messages_ids = [msg.message_id for msg in new_messages]
            for msg in previous_export:
                if msg["message_id"] in new_messages_ids:
                    continue
                messages.append(msg)

        else:
            log.warning(f"{engagement_db_dataset} previous export file does not exist, "
                        f"performing a full download ...")

            full_download_filter = lambda q: q \
                .where("dataset", "==", engagement_db_dataset)

            new_messages.extend(engagement_db.get_messages(filter=full_download_filter))

        # Convert new Message objects to Json serializable dicts
        for msg in new_messages:
            msg = msg.to_dict()
            msg["timestamp"] = TimeUtils.datetime_to_utc_iso_string(msg["timestamp"]),
            msg["last_updated"] = TimeUtils.datetime_to_utc_iso_string(msg["last_updated"])
            messages.append(msg)

        engagement_db_dataset_messages_map[engagement_db_dataset] = messages

        # Export project engagement_dataset files
        with open(previous_export_file_path, 'w') as f:
            json.dump(messages, f, indent=2)

    return engagement_db_dataset_messages_map

#TODO: Filter Messages
def filter_messages():
    return None

#TODO: Fold messages by uid
def fold_messages_by_uid():
    return None

